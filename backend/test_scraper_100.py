"""æµ‹è¯•100ä¸ªæ’­å®¢çš„æŠ“å–åŠŸèƒ½ï¼ˆå¸¦åçˆ¬è™«ç­–ç•¥ï¼‰"""
import asyncio
import json
from datetime import datetime, date
from typing import List, Dict
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.session import AsyncSessionFactory
from app.models.podcast import Podcast, PodcastDailyMetric
from app.services.scraper_service import PodcastScraper
from app.services.anti_scraping import create_anti_scraping_manager
from loguru import logger


class ScraperTester:
    """çˆ¬è™«æµ‹è¯•å™¨"""
    
    def __init__(self, limit: int = 100):
        self.limit = limit
        self.results: List[Dict] = []
        self.stats = {
            "total": 0,
            "successful": 0,
            "failed": 0,
            "errors": {},
            "start_time": None,
            "end_time": None,
            "duration": None
        }
    
    async def test_single_podcast(
        self,
        scraper: PodcastScraper,
        podcast: Podcast,
        index: int,
        session: AsyncSession,
        save_to_db: bool = True
    ) -> Dict:
        """æµ‹è¯•å•ä¸ªæ’­å®¢å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        result = {
            "index": index,
            "xyz_id": podcast.xyz_id,
            "name": podcast.name,
            "success": False,
            "subscriber_count": None,
            "saved_to_db": False,
            "error": None,
            "error_type": None
        }
        
        try:
            logger.info(f"[{index}/{self.limit}] æµ‹è¯•: {podcast.name} ({podcast.xyz_id})")
            subscriber_count = await scraper.scrape_subscriber_count(podcast.xyz_id)
            
            if subscriber_count is not None:
                result["success"] = True
                result["subscriber_count"] = subscriber_count
                logger.success(f"[{index}/{self.limit}] âœ… {podcast.name}: {subscriber_count:,} è®¢é˜…è€…")
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if save_to_db:
                    try:
                        today = date.today()
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä»Šæ—¥æ•°æ®
                        existing_metric = await session.execute(
                            select(PodcastDailyMetric).where(
                                PodcastDailyMetric.podcast_id == podcast.id,
                                PodcastDailyMetric.snapshot_date == today
                            )
                        )
                        existing = existing_metric.scalar_one_or_none()
                        
                        if existing:
                            # æ›´æ–°ç°æœ‰è®°å½•
                            existing.subscriber_count = subscriber_count
                            logger.info(f"[{index}/{self.limit}] ğŸ“ æ›´æ–°æ•°æ®åº“è®°å½•: {podcast.name} ({subscriber_count:,})")
                        else:
                            # åˆ›å»ºæ–°è®°å½•
                            metric = PodcastDailyMetric(
                                podcast_id=podcast.id,
                                snapshot_date=today,
                                subscriber_count=subscriber_count
                            )
                            session.add(metric)
                            logger.info(f"[{index}/{self.limit}] ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“: {podcast.name} ({subscriber_count:,})")
                        
                        await session.commit()
                        result["saved_to_db"] = True
                    except Exception as db_error:
                        logger.error(f"[{index}/{self.limit}] âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {db_error}")
                        await session.rollback()
                        result["error"] = f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {db_error}"
            else:
                result["error"] = "æœªè·å–åˆ°è®¢é˜…æ•°"
                result["error_type"] = "no_data"
                logger.warning(f"[{index}/{self.limit}] âš ï¸ {podcast.name}: æœªè·å–åˆ°è®¢é˜…æ•°")
        
        except Exception as e:
            result["error"] = str(e)
            result["error_type"] = type(e).__name__
            logger.error(f"[{index}/{self.limit}] âŒ {podcast.name}: {e}")
        
        return result
    
    async def test_batch(self, max_concurrent: int = 3) -> Dict:
        """æ‰¹é‡æµ‹è¯•"""
        self.stats["start_time"] = datetime.now()
        logger.info(f"å¼€å§‹æµ‹è¯• {self.limit} ä¸ªæ’­å®¢ï¼Œå¹¶å‘æ•°: {max_concurrent}")
        
        # åˆ›å»ºåçˆ¬è™«ç®¡ç†å™¨ï¼ˆä½¿ç”¨ä¿å®ˆç­–ç•¥ï¼‰
        anti_scraping_config = {
            "rate_limiter": {
                "max_requests": 10,  # æ¯åˆ†é’Ÿ10ä¸ªè¯·æ±‚
                "time_window": 60
            },
            "request_delay": {
                "min_delay": 3.0,    # 3-6ç§’å»¶è¿Ÿ
                "max_delay": 6.0,
                "base_delay": 4.0
            },
            "retry_strategy": {
                "max_attempts": 3,
                "initial_delay": 2.0,
                "max_delay": 30.0,
                "backoff_factor": 2.0,
                "jitter": True
            }
        }
        anti_scraping = create_anti_scraping_manager(anti_scraping_config)
        
        # ä»æ•°æ®åº“è·å–æ’­å®¢åˆ—è¡¨
        async with AsyncSessionFactory() as session:
            result = await session.execute(
                select(Podcast).limit(self.limit)
            )
            podcasts = result.scalars().all()
            self.stats["total"] = len(podcasts)
            logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(podcasts)} ä¸ªæ’­å®¢")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def test_with_semaphore(podcast: Podcast, index: int):
            async with semaphore:
                async with AsyncSessionFactory() as session:
                    scraper = PodcastScraper(session, anti_scraping_manager=anti_scraping)
                    try:
                        result = await self.test_single_podcast(
                            scraper, podcast, index, session, save_to_db=True
                        )
                        return result
                    finally:
                        await scraper.close()
        
        # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
        tasks = [
            test_with_semaphore(podcast, i + 1)
            for i, podcast in enumerate(podcasts)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.results.append({
                    "index": i + 1,
                    "xyz_id": podcasts[i].xyz_id if i < len(podcasts) else "N/A",
                    "name": podcasts[i].name if i < len(podcasts) else "N/A",
                    "success": False,
                    "error": str(result),
                    "error_type": type(result).__name__
                })
            else:
                self.results.append(result)
        
        # ç»Ÿè®¡
        self.stats["end_time"] = datetime.now()
        self.stats["duration"] = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
        
        saved_count = 0
        for result in self.results:
            if result.get("success"):
                self.stats["successful"] += 1
                if result.get("saved_to_db"):
                    saved_count += 1
            else:
                self.stats["failed"] += 1
                error_type = result.get("error_type", "unknown")
                self.stats["errors"][error_type] = self.stats["errors"].get(error_type, 0) + 1
        
        self.stats["saved_to_db"] = saved_count
        
        return self.stats
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_info": {
                "total_tested": self.stats["total"],
                "successful": self.stats["successful"],
                "failed": self.stats["failed"],
                "saved_to_db": self.stats.get("saved_to_db", 0),
                "success_rate": f"{self.stats['successful']/self.stats['total']*100:.2f}%" if self.stats["total"] > 0 else "0%",
                "duration_seconds": self.stats["duration"],
                "duration_formatted": f"{int(self.stats['duration']//60)}åˆ†{int(self.stats['duration']%60)}ç§’" if self.stats["duration"] else None,
                "avg_time_per_request": f"{self.stats['duration']/self.stats['total']:.2f}ç§’" if self.stats["total"] > 0 and self.stats["duration"] else None
            },
            "error_statistics": self.stats["errors"],
            "sample_results": {
                "successful": [r for r in self.results if r.get("success")][:10],
                "failed": [r for r in self.results if not r.get("success")][:10]
            },
            "subscriber_count_stats": self._calculate_subscriber_stats()
        }
        return report
    
    def _calculate_subscriber_stats(self) -> Dict:
        """è®¡ç®—è®¢é˜…æ•°ç»Ÿè®¡"""
        successful_results = [r for r in self.results if r.get("success") and r.get("subscriber_count")]
        if not successful_results:
            return {}
        
        counts = [r["subscriber_count"] for r in successful_results]
        return {
            "count": len(counts),
            "min": min(counts),
            "max": max(counts),
            "avg": int(sum(counts) / len(counts)),
            "median": sorted(counts)[len(counts) // 2]
        }


async def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("æ’­å®¢è®¢é˜…æ•°æŠ“å–æµ‹è¯• - 100ä¸ªæ’­å®¢")
    print("="*80)
    print()
    print("é…ç½®:")
    print("  - æµ‹è¯•æ•°é‡: 100ä¸ªæ’­å®¢")
    print("  - å¹¶å‘æ•°: 3ä¸ª")
    print("  - åçˆ¬è™«ç­–ç•¥: æ¯åˆ†é’Ÿ10ä¸ªè¯·æ±‚ï¼Œ3-6ç§’å»¶è¿Ÿ")
    print()
    
    tester = ScraperTester(limit=100)
    
    try:
        # æ‰§è¡Œæµ‹è¯•
        stats = await tester.test_batch(max_concurrent=3)
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\n" + "="*80)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        report = tester.generate_report()
        
        print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {report['test_info']['total_tested']}")
        print(f"  æˆåŠŸæ•°: {report['test_info']['successful']}")
        print(f"  å¤±è´¥æ•°: {report['test_info']['failed']}")
        print(f"  æˆåŠŸç‡: {report['test_info']['success_rate']}")
        print(f"  ä¿å­˜åˆ°æ•°æ®åº“: {stats.get('saved_to_db', 0)} æ¡")
        print(f"  æ€»è€—æ—¶: {report['test_info']['duration_formatted']}")
        print(f"  å¹³å‡è€—æ—¶: {report['test_info']['avg_time_per_request']} / è¯·æ±‚")
        
        if report['error_statistics']:
            print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
            for error_type, count in report['error_statistics'].items():
                print(f"  {error_type}: {count} æ¬¡")
        
        if report['subscriber_count_stats']:
            stats = report['subscriber_count_stats']
            print(f"\nğŸ“ˆ è®¢é˜…æ•°ç»Ÿè®¡:")
            print(f"  æœ‰æ•ˆæ•°æ®: {stats['count']} ä¸ª")
            print(f"  æœ€å°å€¼: {stats['min']:,}")
            print(f"  æœ€å¤§å€¼: {stats['max']:,}")
            print(f"  å¹³å‡å€¼: {stats['avg']:,}")
            print(f"  ä¸­ä½æ•°: {stats['median']:,}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = "scraper_100_test_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump({
                "test_time": datetime.now().isoformat(),
                "report": report,
                "detailed_results": tester.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºæˆåŠŸå’Œå¤±è´¥ç¤ºä¾‹
        if report['sample_results']['successful']:
            print(f"\nâœ… æˆåŠŸç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for r in report['sample_results']['successful'][:5]:
                print(f"  - {r['name']}: {r['subscriber_count']:,} è®¢é˜…è€…")
        
        if report['sample_results']['failed']:
            print(f"\nâŒ å¤±è´¥ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for r in report['sample_results']['failed'][:5]:
                print(f"  - {r['name']}: {r.get('error', 'Unknown error')}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¢«å°ç¦çš„è¿¹è±¡
        http_errors = {
            k: v for k, v in report['error_statistics'].items()
            if '403' in k or '429' in k or 'Forbidden' in k or 'TooManyRequests' in k
        }
        if http_errors:
            print(f"\nâš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å¯èƒ½çš„å°ç¦è¿¹è±¡!")
            print(f"  é”™è¯¯ç±»å‹: {http_errors}")
            print(f"  å»ºè®®: é™ä½è¯·æ±‚é¢‘ç‡æˆ–å¢åŠ å»¶è¿Ÿ")
        else:
            print(f"\nâœ… æœªæ£€æµ‹åˆ°å°ç¦è¿¹è±¡ï¼Œåçˆ¬è™«ç­–ç•¥æ­£å¸¸å·¥ä½œ")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())

