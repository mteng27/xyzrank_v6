# åçˆ¬è™«ç­–ç•¥æŒ‡å—

> æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•é…ç½®å’Œä½¿ç”¨åçˆ¬è™«ç­–ç•¥ï¼Œé¿å…è¢«ç›®æ ‡ç½‘ç«™å°ç¦ã€‚

---

## ğŸ“‹ ç­–ç•¥æ¦‚è§ˆ

æˆ‘ä»¬å®ç°äº†ä»¥ä¸‹åçˆ¬è™«ç­–ç•¥ï¼š

### 1. è¯·æ±‚é¢‘ç‡æ§åˆ¶ (Rate Limiting)
- **ä»¤ç‰Œæ¡¶ç®—æ³•**ï¼šæ§åˆ¶æ—¶é—´çª—å£å†…çš„è¯·æ±‚æ•°é‡
- **é»˜è®¤é…ç½®**ï¼šæ¯åˆ†é’Ÿæœ€å¤š 10 ä¸ªè¯·æ±‚
- **å¯é…ç½®**ï¼š`max_requests` å’Œ `time_window`

### 2. User-Agent è½®æ¢
- **å¤šæµè§ˆå™¨æ”¯æŒ**ï¼šChromeã€Safariã€Firefoxã€Edge
- **å¤šå¹³å°æ”¯æŒ**ï¼šWindowsã€macOS
- **ç­–ç•¥**ï¼šéšæœºé€‰æ‹©æˆ–é¡ºåºè½®æ¢

### 3. è¯·æ±‚é—´éš”éšæœºåŒ–
- **æ­£æ€åˆ†å¸ƒå»¶è¿Ÿ**ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸º
- **é»˜è®¤é…ç½®**ï¼š3-6 ç§’éšæœºå»¶è¿Ÿ
- **å¯é…ç½®**ï¼š`min_delay`ã€`max_delay`ã€`base_delay`

### 4. è¯·æ±‚å¤´éšæœºåŒ–
- **çœŸå®æµè§ˆå™¨è¯·æ±‚å¤´**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„ HTTP å¤´
- **å¹³å°ç›¸å…³**ï¼šæ ¹æ® User-Agent è‡ªåŠ¨è°ƒæ•´
- **å®‰å…¨ç›¸å…³**ï¼šåŒ…å« `Sec-Fetch-*` ç­‰ç°ä»£æµè§ˆå™¨å¤´

### 5. é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
- **è‡ªåŠ¨é‡è¯•**ï¼šç½‘ç»œé”™è¯¯æ—¶è‡ªåŠ¨é‡è¯•
- **æŒ‡æ•°é€€é¿**ï¼šå»¶è¿Ÿæ—¶é—´é€æ¸å¢åŠ 
- **éšæœºæŠ–åŠ¨**ï¼šé¿å…é›·ç¾¤æ•ˆåº”
- **é»˜è®¤é…ç½®**ï¼šæœ€å¤šé‡è¯• 3 æ¬¡

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```python
from app.services.scraper_service import PodcastScraper
from app.db.session import get_db_session

# ä½¿ç”¨é»˜è®¤åçˆ¬è™«ç­–ç•¥
async with get_db_session() as session:
    scraper = PodcastScraper(session)
    # è‡ªåŠ¨åº”ç”¨åçˆ¬è™«ç­–ç•¥
    subscriber_count = await scraper.scrape_subscriber_count(xyz_id)
```

### è‡ªå®šä¹‰é…ç½®

```python
from app.services.scraper_service import PodcastScraper
from app.services.anti_scraping import create_anti_scraping_manager

# è‡ªå®šä¹‰åçˆ¬è™«é…ç½®
custom_config = {
    "rate_limiter": {
        "max_requests": 5,   # æ¯åˆ†é’Ÿæœ€å¤š5ä¸ªè¯·æ±‚ï¼ˆæ›´ä¿å®ˆï¼‰
        "time_window": 60
    },
    "request_delay": {
        "min_delay": 5.0,    # æœ€å°å»¶è¿Ÿ5ç§’
        "max_delay": 10.0,   # æœ€å¤§å»¶è¿Ÿ10ç§’
        "base_delay": 7.0    # åŸºç¡€å»¶è¿Ÿ7ç§’
    },
    "retry_strategy": {
        "max_attempts": 5,   # æœ€å¤šé‡è¯•5æ¬¡
        "initial_delay": 3.0,
        "max_delay": 60.0,
        "backoff_factor": 2.0,
        "jitter": True
    }
}

# åˆ›å»ºè‡ªå®šä¹‰åçˆ¬è™«ç®¡ç†å™¨
anti_scraping = create_anti_scraping_manager(custom_config)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
async with get_db_session() as session:
    scraper = PodcastScraper(session, anti_scraping_manager=anti_scraping)
    subscriber_count = await scraper.scrape_subscriber_count(xyz_id)
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### ä¿å®ˆç­–ç•¥ï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```python
CONSERVATIVE_CONFIG = {
    "rate_limiter": {
        "max_requests": 5,   # æ¯åˆ†é’Ÿ5ä¸ªè¯·æ±‚
        "time_window": 60
    },
    "request_delay": {
        "min_delay": 5.0,    # 5-10ç§’å»¶è¿Ÿ
        "max_delay": 10.0,
        "base_delay": 7.0
    },
    "retry_strategy": {
        "max_attempts": 3,
        "initial_delay": 3.0,
        "max_delay": 60.0,
        "backoff_factor": 2.0,
        "jitter": True
    }
}
```

### å¹³è¡¡ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰

```python
BALANCED_CONFIG = {
    "rate_limiter": {
        "max_requests": 10,  # æ¯åˆ†é’Ÿ10ä¸ªè¯·æ±‚
        "time_window": 60
    },
    "request_delay": {
        "min_delay": 3.0,    # 3-6ç§’å»¶è¿Ÿ
        "max_delay": 6.0,
        "base_delay": 4.0
    },
    "retry_strategy": {
        "max_attempts": 3,
        "initial_delay": 2.0,
        "max_delay": 30.0,
        "backoff_factor": 2.0,
        "jitter": True
    }
}
```

### å¿«é€Ÿç­–ç•¥ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼Œä¸æ¨èç”Ÿäº§ç¯å¢ƒï¼‰

```python
FAST_CONFIG = {
    "rate_limiter": {
        "max_requests": 20,  # æ¯åˆ†é’Ÿ20ä¸ªè¯·æ±‚
        "time_window": 60
    },
    "request_delay": {
        "min_delay": 1.0,    # 1-3ç§’å»¶è¿Ÿ
        "max_delay": 3.0,
        "base_delay": 2.0
    },
    "retry_strategy": {
        "max_attempts": 2,
        "initial_delay": 1.0,
        "max_delay": 10.0,
        "backoff_factor": 2.0,
        "jitter": True
    }
}
```

---

## ğŸ“Š ç›‘æ§å’Œç»Ÿè®¡

### è·å–ç»Ÿè®¡ä¿¡æ¯

```python
# è·å–åçˆ¬è™«ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
stats = scraper.anti_scraping.get_stats()
print(stats)
# {
#     "rate_limiter": {
#         "current_requests": 5,
#         "max_requests": 10,
#         "time_window": 60,
#         "wait_time": 0.0
#     },
#     "user_agent": {
#         "current_index": 3,
#         "total_agents": 10
#     }
# }
```

---

## ğŸ›¡ï¸ æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®

- âœ… ä½¿ç”¨**ä¿å®ˆç­–ç•¥**ï¼ˆæ¯åˆ†é’Ÿ5ä¸ªè¯·æ±‚ï¼‰
- âœ… è®¾ç½®è¾ƒé•¿çš„å»¶è¿Ÿï¼ˆ5-10ç§’ï¼‰
- âœ… å¯ç”¨é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3-5æ¬¡ï¼‰
- âœ… ç›‘æ§è¯·æ±‚é¢‘ç‡å’Œé”™è¯¯ç‡

### 2. æ‰¹é‡æŠ“å–

```python
# æ‰¹é‡æŠ“å–æ—¶ï¼Œå»ºè®®ï¼š
# 1. ä½¿ç”¨è¾ƒä½çš„å¹¶å‘æ•°ï¼ˆæœ€å¤š3-5ä¸ªå¹¶å‘ï¼‰
# 2. æ¯ä¸ªè¯·æ±‚ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
# 3. åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§æŠ“å–å¤ªå¤š

async def scrape_batch(podcasts, max_concurrent=3):
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def scrape_one(podcast):
        async with semaphore:
            # åçˆ¬è™«ç­–ç•¥ä¼šè‡ªåŠ¨åº”ç”¨
            return await scraper.scrape_subscriber_count(podcast.xyz_id)
    
    tasks = [scrape_one(p) for p in podcasts]
    return await asyncio.gather(*tasks)
```

### 3. é”™è¯¯å¤„ç†

```python
# å¦‚æœé‡åˆ° 429 (Too Many Requests) æˆ– 403 (Forbidden)ï¼š
# 1. ç«‹å³åœæ­¢è¯·æ±‚
# 2. å¢åŠ å»¶è¿Ÿæ—¶é—´
# 3. å‡å°‘å¹¶å‘æ•°
# 4. ç­‰å¾…ä¸€æ®µæ—¶é—´åå†ç»§ç»­

if response.status_code == 429:
    logger.warning("é‡åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´...")
    await asyncio.sleep(300)  # ç­‰å¾…5åˆ†é’Ÿ
    # è°ƒæ•´é…ç½®ï¼Œé™ä½è¯·æ±‚é¢‘ç‡
```

### 4. æ—¥å¿—è®°å½•

```python
# è®°å½•æ‰€æœ‰è¯·æ±‚ï¼Œä¾¿äºåˆ†æ
logger.info(f"è¯·æ±‚ {url}, User-Agent: {headers['User-Agent']}")
logger.info(f"å»¶è¿Ÿ: {delay:.2f}ç§’")
logger.info(f"é¢‘ç‡é™åˆ¶: {stats['rate_limiter']['current_requests']}/{stats['rate_limiter']['max_requests']}")
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä¸è¦è¿‡äºæ¿€è¿›**
   - å³ä½¿æœ‰åçˆ¬è™«ç­–ç•¥ï¼Œä¹Ÿä¸è¦è®¾ç½®è¿‡é«˜çš„è¯·æ±‚é¢‘ç‡
   - å»ºè®®æ¯åˆ†é’Ÿä¸è¶…è¿‡ 10 ä¸ªè¯·æ±‚

2. **ç›‘æ§é”™è¯¯ç‡**
   - å¦‚æœé”™è¯¯ç‡çªç„¶å¢åŠ ï¼Œå¯èƒ½æ˜¯è¢«å°ç¦çš„å‰å…†
   - ç«‹å³é™ä½è¯·æ±‚é¢‘ç‡

3. **éµå®ˆ robots.txt**
   - æ£€æŸ¥ç›®æ ‡ç½‘ç«™çš„ robots.txt
   - éµå®ˆçˆ¬å–è§„åˆ™

4. **ä½¿ç”¨ä»£ç†ï¼ˆå¯é€‰ï¼‰**
   - å¦‚æœéœ€è¦æ›´é«˜çš„è¯·æ±‚é¢‘ç‡ï¼Œè€ƒè™‘ä½¿ç”¨ä»£ç†æ± 
   - å½“å‰å®ç°ä¸åŒ…å«ä»£ç†ï¼Œéœ€è¦æ—¶å¯ä»¥æ‰©å±•

5. **å®šæœŸæ›´æ–° User-Agent**
   - å®šæœŸæ›´æ–° User-Agent åˆ—è¡¨ï¼Œä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
   - é¿å…ä½¿ç”¨è¿‡æ—¶çš„æµè§ˆå™¨ç‰ˆæœ¬

---

## ğŸ”§ æ‰©å±•åŠŸèƒ½

### æ·»åŠ ä»£ç†æ”¯æŒ

```python
# å¯ä»¥åœ¨ AntiScrapingManager ä¸­æ·»åŠ ä»£ç†è½®æ¢
class ProxyRotator:
    def __init__(self, proxies: List[str]):
        self.proxies = proxies
        self.current_index = 0
    
    def get_next(self) -> str:
        proxy = self.proxies[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.proxies)
        return proxy
```

### æ·»åŠ  Cookie ç®¡ç†

```python
# å¦‚æœéœ€è¦ç™»å½•æˆ–ä¿æŒä¼šè¯
class CookieManager:
    def __init__(self):
        self.cookies = {}
    
    def update(self, cookies: Dict):
        self.cookies.update(cookies)
    
    def get(self) -> Dict:
        return self.cookies
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

1. **å¹¶å‘æ§åˆ¶**ï¼šä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
2. **æ‰¹é‡å¤„ç†**ï¼šåˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤š
3. **ç¼“å­˜**ï¼šç¼“å­˜å·²æŠ“å–çš„æ•°æ®ï¼Œé¿å…é‡å¤è¯·æ±‚
4. **å¢é‡æ›´æ–°**ï¼šåªæŠ“å–éœ€è¦æ›´æ–°çš„æ’­å®¢

---

## ğŸ“ é…ç½®ç¤ºä¾‹

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶
SCRAPER_MAX_REQUESTS_PER_MINUTE=10
SCRAPER_MIN_DELAY=3.0
SCRAPER_MAX_DELAY=6.0
SCRAPER_MAX_RETRIES=3
```

### ä»£ç ä¸­ä½¿ç”¨

```python
import os
from app.services.anti_scraping import create_anti_scraping_manager

config = {
    "rate_limiter": {
        "max_requests": int(os.getenv("SCRAPER_MAX_REQUESTS_PER_MINUTE", 10)),
        "time_window": 60
    },
    "request_delay": {
        "min_delay": float(os.getenv("SCRAPER_MIN_DELAY", 3.0)),
        "max_delay": float(os.getenv("SCRAPER_MAX_DELAY", 6.0)),
        "base_delay": 4.0
    },
    "retry_strategy": {
        "max_attempts": int(os.getenv("SCRAPER_MAX_RETRIES", 3)),
        "initial_delay": 2.0,
        "max_delay": 30.0,
        "backoff_factor": 2.0,
        "jitter": True
    }
}

anti_scraping = create_anti_scraping_manager(config)
```

---

**æœ€åæ›´æ–°**: 2025-01-XX


