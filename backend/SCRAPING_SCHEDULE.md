# çˆ¬è™«è°ƒåº¦ç­–ç•¥è¯´æ˜

## ğŸ“… è°ƒåº¦æ–¹æ¡ˆ

### ç›®æ ‡
- **ä¸€å‘¨å†…å®Œæˆæ‰€æœ‰7000ä¸ªæ’­å®¢çš„çˆ¬å–**
- **æ¯å¤©çˆ¬å–çº¦1000ä¸ªæ’­å®¢**ï¼ˆ7000 Ã· 7 â‰ˆ 1000ï¼‰

### å®ç°ç­–ç•¥

#### 1. åŸºäºå“ˆå¸Œå€¼çš„è½®è¯¢æœºåˆ¶
ä½¿ç”¨æ’­å®¢IDçš„å“ˆå¸Œå€¼å¯¹å‘¨æœŸå¤©æ•°å–æ¨¡ï¼Œç¡®ä¿ï¼š
- æ¯ä¸ªæ’­å®¢åœ¨7å¤©å†…ä¼šè¢«çˆ¬å–ä¸€æ¬¡
- æ¯å¤©çˆ¬å–ä¸åŒçš„æ’­å®¢é›†åˆ
- åˆ†å¸ƒå‡åŒ€ï¼Œé¿å…é›†ä¸­åœ¨æŸäº›æ’­å®¢

**ç®—æ³•**ï¼š
```python
day_of_cycle = today.toordinal() % 7  # 0-6
podcasts_to_scrape = [
    p for p in all_podcasts
    if hash(p.id) % 7 == day_of_cycle
]
```

#### 2. æ‰§è¡Œæ—¶é—´
- **é»˜è®¤æ—¶é—´**ï¼šæ¯å¤©å‡Œæ™¨ 2:00
- **å¯é…ç½®**ï¼šåœ¨ `scheduler.py` ä¸­ä¿®æ”¹ `CronTrigger`

#### 3. å¹¶å‘æ§åˆ¶
- **å¹¶å‘æ•°**ï¼š3ä¸ªï¼ˆé€šè¿‡ `AntiScrapingManager` æ§åˆ¶ï¼‰
- **è¯·æ±‚é¢‘ç‡**ï¼šæ¯åˆ†é’Ÿ10ä¸ªè¯·æ±‚
- **å»¶è¿Ÿ**ï¼š3-6ç§’éšæœºå»¶è¿Ÿ

## ğŸ”§ é…ç½®é€‰é¡¹

### ä¿®æ”¹æ¯å¤©çˆ¬å–æ•°é‡
åœ¨ `scheduler.py` çš„ `daily_scrape_task()` ä¸­ä¿®æ”¹ï¼š
```python
scrape_run = await scraper.scrape_podcasts_batch(
    batch_size=1000,  # ä¿®æ”¹è¿™ä¸ªå€¼
    days_in_cycle=7
)
```

### ä¿®æ”¹å‘¨æœŸå¤©æ•°
```python
scrape_run = await scraper.scrape_podcasts_batch(
    batch_size=1000,
    days_in_cycle=7  # ä¿®æ”¹è¿™ä¸ªå€¼ï¼ˆä¾‹å¦‚æ”¹ä¸º14å¤©ï¼‰
)
```

### ä¿®æ”¹æ‰§è¡Œæ—¶é—´
åœ¨ `scheduler.py` çš„ `setup_scheduler()` ä¸­ä¿®æ”¹ï¼š
```python
scheduler.add_job(
    daily_scrape_task,
    trigger=CronTrigger(hour=2, minute=0),  # ä¿®æ”¹è¿™é‡Œ
    ...
)
```

## ğŸ“Š é¢„æœŸæ•ˆæœ

### ç¬¬ä¸€å‘¨
- **ç¬¬1å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 0ï¼‰
- **ç¬¬2å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 1ï¼‰
- **ç¬¬3å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 2ï¼‰
- **ç¬¬4å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 3ï¼‰
- **ç¬¬5å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 4ï¼‰
- **ç¬¬6å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 5ï¼‰
- **ç¬¬7å¤©**ï¼šçˆ¬å–çº¦1000ä¸ªæ’­å®¢ï¼ˆIDå“ˆå¸Œå€¼ % 7 == 6ï¼‰

### ç¬¬äºŒå‘¨åŠä»¥å
- é‡å¤ç¬¬ä¸€å‘¨çš„å¾ªç¯
- æ¯ä¸ªæ’­å®¢æ¯å‘¨æ›´æ–°ä¸€æ¬¡æ•°æ®

## âš™ï¸ é«˜çº§é€‰é¡¹

### æ–¹æ¡ˆAï¼šæ¯å¤©å›ºå®šæ•°é‡ï¼ˆå½“å‰å®ç°ï¼‰
- ä¼˜ç‚¹ï¼šç®€å•ï¼Œå¯é¢„æµ‹
- ç¼ºç‚¹ï¼šå¦‚æœæ’­å®¢æ€»æ•°å˜åŒ–ï¼Œåˆ†å¸ƒå¯èƒ½ä¸å‡åŒ€

### æ–¹æ¡ˆBï¼šåŸºäºä¸Šæ¬¡çˆ¬å–æ—¶é—´
ä¼˜å…ˆçˆ¬å–é•¿æ—¶é—´æœªæ›´æ–°çš„æ’­å®¢ï¼š
```python
# è·å–7å¤©å‰æœªæ›´æ–°çš„æ’­å®¢
cutoff_date = date.today() - timedelta(days=7)
podcasts_to_scrape = await session.execute(
    select(Podcast)
    .outerjoin(
        PodcastDailyMetric,
        (Podcast.id == PodcastDailyMetric.podcast_id) &
        (PodcastDailyMetric.snapshot_date >= cutoff_date)
    )
    .where(PodcastDailyMetric.id.is_(None))
    .limit(1000)
)
```

### æ–¹æ¡ˆCï¼šä¼˜å…ˆçº§é˜Ÿåˆ—
æ ¹æ®æ’­å®¢çš„è®¢é˜…æ•°æˆ–é‡è¦æ€§è®¾ç½®ä¼˜å…ˆçº§ï¼š
- é«˜è®¢é˜…æ•°æ’­å®¢ï¼šæ¯å¤©æ›´æ–°
- ä¸­ç­‰è®¢é˜…æ•°æ’­å®¢ï¼šæ¯å‘¨æ›´æ–°
- ä½è®¢é˜…æ•°æ’­å®¢ï¼šæ¯æœˆæ›´æ–°

## ğŸš€ æ‰‹åŠ¨è§¦å‘

### æµ‹è¯•åˆ†æ‰¹çˆ¬å–
```python
from app.db.session import AsyncSessionFactory
from app.services.scraper_service import PodcastScraper
from app.services.anti_scraping import create_anti_scraping_manager

async with AsyncSessionFactory() as session:
    scraper = PodcastScraper(session, create_anti_scraping_manager())
    try:
        scrape_run = await scraper.scrape_podcasts_batch(
            batch_size=100,  # æµ‹è¯•æ—¶ç”¨è¾ƒå°çš„æ•°é‡
            days_in_cycle=7
        )
        print(f"å®Œæˆ: {scrape_run.successful_count}/{scrape_run.total_podcasts}")
    finally:
        await scraper.close()
```

### é€šè¿‡APIè§¦å‘
```bash
curl -X POST http://localhost:8000/api/scraper/run
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ’åè®¡ç®—**ï¼šæ¯å¤©çˆ¬å–å®Œæˆåï¼Œä¼šåŸºäºå½“å¤©æ‰€æœ‰å·²çˆ¬å–çš„æ’­å®¢è®¡ç®—æ’å
2. **æ•°æ®ä¸€è‡´æ€§**ï¼šæ’ååŸºäºåŒä¸€å¤©çš„æ•°æ®ï¼Œä¿è¯ä¸€è‡´æ€§
3. **å¤±è´¥é‡è¯•**ï¼šå¤±è´¥çš„æ’­å®¢ä¼šåœ¨ä¸‹ä¸€ä¸ªå‘¨æœŸè‡ªåŠ¨é‡è¯•
4. **èµ„æºæ¶ˆè€—**ï¼šæ¯å¤©çº¦1000ä¸ªæ’­å®¢ï¼ŒæŒ‰3-6ç§’å»¶è¿Ÿï¼Œé¢„è®¡éœ€è¦1-2å°æ—¶å®Œæˆ

