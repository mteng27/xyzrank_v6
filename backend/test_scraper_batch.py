"""æ‰¹é‡æµ‹è¯•æ’­å®¢æ•°æ®æŠ“å– - æ£€æµ‹100ä¸ªç½‘å€çš„æ•°æ®å¯ç”¨æ€§"""
import asyncio
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from httpx import AsyncClient
from bs4 import BeautifulSoup
import json
import sys

# ç®€å•çš„æ—¥å¿—å‡½æ•°ï¼ˆå¦‚æœloguruæœªå®‰è£…ï¼‰
try:
    from loguru import logger
    # é…ç½®æ—¥å¿—
    logger.add("scraper_test.log", rotation="10 MB", level="INFO")
except ImportError:
    class SimpleLogger:
        def info(self, msg): print(f"[INFO] {msg}")
        def warning(self, msg): print(f"[WARN] {msg}")
        def error(self, msg): print(f"[ERROR] {msg}")
    logger = SimpleLogger()


class BatchScraperTester:
    """æ‰¹é‡æŠ“å–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.client = AsyncClient(
            timeout=30.0,
            follow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            }
        )
        self.results: List[Dict] = []
    
    async def test_single_podcast(self, podcast_data: Dict) -> Dict:
        """
        æµ‹è¯•å•ä¸ªæ’­å®¢çš„æ•°æ®å¯ç”¨æ€§
        
        Args:
            podcast_data: åŒ…å« xyz_id å’Œ url çš„å­—å…¸
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        xyz_id = podcast_data['xyz_id']
        url = podcast_data['url']
        
        result = {
            "xyz_id": xyz_id,
            "url": url,
            "name": podcast_data.get('name'),
            "accessible": False,
            "has_title": False,
            "has_rss": False,
            "has_cover": False,
            "has_description": False,
            "status_code": None,
            "error": None,
            "data": {}
        }
        
        try:
            # æµ‹è¯•URLå¯è®¿é—®æ€§
            response = await self.client.get(url)
            result["status_code"] = response.status_code
            result["accessible"] = response.status_code == 200
            
            if not result["accessible"]:
                result["error"] = f"HTTP {response.status_code}"
                return result
            
            # è§£æé¡µé¢å†…å®¹
            soup = BeautifulSoup(response.text, "html.parser")
            
            # æ£€æŸ¥æ ‡é¢˜
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.get_text().strip()
                result["has_title"] = True
                result["data"]["title"] = title
            
            # æ£€æŸ¥RSSé“¾æ¥
            rss_link = soup.find("link", {"type": "application/rss+xml"})
            if not rss_link:
                # å°è¯•å…¶ä»–æ–¹å¼æŸ¥æ‰¾RSS
                rss_link = soup.find("a", href=lambda x: x and "rss" in x.lower())
            
            if rss_link:
                result["has_rss"] = True
                href = rss_link.get("href") or (rss_link.text if hasattr(rss_link, 'text') else None)
                result["data"]["rss_url"] = href
            
            # æ£€æŸ¥å°é¢å›¾
            og_image = soup.find("meta", {"property": "og:image"})
            if og_image:
                result["has_cover"] = True
                result["data"]["cover_url"] = og_image.get("content")
            else:
                # å°è¯•æŸ¥æ‰¾å…¶ä»–å›¾ç‰‡æ ‡ç­¾
                img_tag = soup.find("img", class_=lambda x: x and "cover" in str(x).lower())
                if img_tag:
                    result["has_cover"] = True
                    result["data"]["cover_url"] = img_tag.get("src")
            
            # æ£€æŸ¥æè¿°
            description_tag = soup.find("meta", {"name": "description"})
            if description_tag:
                result["has_description"] = True
                result["data"]["description"] = description_tag.get("content")
            else:
                # å°è¯•æŸ¥æ‰¾å…¶ä»–æè¿°å…ƒç´ 
                desc_tag = soup.find("p", class_=lambda x: x and "description" in str(x).lower())
                if desc_tag:
                    result["has_description"] = True
                    result["data"]["description"] = desc_tag.get_text()[:200]  # é™åˆ¶é•¿åº¦
            
            # å°è¯•æŸ¥æ‰¾è®¢é˜…è€…æ•°é‡ï¼ˆå¦‚æœé¡µé¢ä¸­æœ‰ï¼‰
            # è¿™éœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
            subscriber_elements = soup.find_all(string=lambda text: text and "è®¢é˜…" in text)
            if subscriber_elements:
                result["data"]["has_subscriber_info"] = True
            
            logger.info(f"âœ… {xyz_id}: å¯è®¿é—®, æ ‡é¢˜={result['has_title']}, RSS={result['has_rss']}")
            
        except Exception as e:
            result["error"] = str(e)
            logger.error(f"âŒ {xyz_id}: {e}")
        
        return result
    
    async def test_batch(self, podcasts: List[Dict], limit: int = 100) -> List[Dict]:
        """
        æ‰¹é‡æµ‹è¯•æ’­å®¢
        
        Args:
            podcasts: æ’­å®¢æ•°æ®åˆ—è¡¨ï¼ˆåŒ…å« xyz_id å’Œ urlï¼‰
            limit: æµ‹è¯•æ•°é‡é™åˆ¶
        
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        test_podcasts = podcasts[:limit]
        logger.info(f"å¼€å§‹æµ‹è¯• {len(test_podcasts)} ä¸ªæ’­å®¢...")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
        semaphore = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘è¯·æ±‚
        
        async def test_with_semaphore(podcast_data: Dict):
            async with semaphore:
                return await self.test_single_podcast(podcast_data)
        
        # æ‰¹é‡æµ‹è¯•
        tasks = [test_with_semaphore(podcast) for podcast in test_podcasts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        self.results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                podcast = test_podcasts[i]
                self.results.append({
                    "xyz_id": podcast.get('xyz_id', 'N/A'),
                    "url": podcast.get('url', 'N/A'),
                    "accessible": False,
                    "error": str(result)
                })
            else:
                self.results.append(result)
        
        return self.results
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total = len(self.results)
        accessible = sum(1 for r in self.results if r.get("accessible", False))
        has_title = sum(1 for r in self.results if r.get("has_title", False))
        has_rss = sum(1 for r in self.results if r.get("has_rss", False))
        has_cover = sum(1 for r in self.results if r.get("has_cover", False))
        has_description = sum(1 for r in self.results if r.get("has_description", False))
        errors = [r for r in self.results if r.get("error")]
        
        report = {
            "total": total,
            "accessible": accessible,
            "accessible_rate": f"{accessible/total*100:.2f}%" if total > 0 else "0%",
            "data_availability": {
                "has_title": has_title,
                "has_title_rate": f"{has_title/total*100:.2f}%" if total > 0 else "0%",
                "has_rss": has_rss,
                "has_rss_rate": f"{has_rss/total*100:.2f}%" if total > 0 else "0%",
                "has_cover": has_cover,
                "has_cover_rate": f"{has_cover/total*100:.2f}%" if total > 0 else "0%",
                "has_description": has_description,
                "has_description_rate": f"{has_description/total*100:.2f}%" if total > 0 else "0%",
            },
            "errors_count": len(errors),
            "error_samples": errors[:10]  # å‰10ä¸ªé”™è¯¯ç¤ºä¾‹
        }
        
        return report
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        await self.client.aclose()


def load_podcast_data_from_excel(file_path: str, limit: int = 100) -> List[Dict]:
    """
    ä»Excelæ–‡ä»¶åŠ è½½æ’­å®¢æ•°æ®ï¼ˆåŒ…æ‹¬IDå’ŒURLï¼‰
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        limit: åŠ è½½æ•°é‡é™åˆ¶
    
    Returns:
        æ’­å®¢æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« xyz_id å’Œ url
    """
    try:
        df = pd.read_excel(file_path)
        logger.info(f"Excelæ–‡ä»¶åˆ—å: {df.columns.tolist()}")
        
        # æå–æ•°æ®
        podcasts = []
        for idx, row in df.head(limit).iterrows():
            # å°è¯•ä»link_urlæå–IDï¼Œæˆ–ä½¿ç”¨album_id
            link_url = str(row.get('link_url', '')).strip()
            album_id = str(row.get('album_id', '')).strip()
            
            # ä»URLä¸­æå–ID
            xyz_id = album_id
            if link_url and link_url != 'nan':
                # å°è¯•ä»URLä¸­æå–ID
                if '/podcast/' in link_url:
                    xyz_id = link_url.split('/podcast/')[-1].split('?')[0].split('#')[0]
                url = link_url
            else:
                # å¦‚æœæ²¡æœ‰URLï¼Œä½¿ç”¨IDæ„å»ºURL
                url = f"https://www.xiaoyuzhoufm.com/podcast/{xyz_id}"
            
            if xyz_id and xyz_id != 'nan':
                podcasts.append({
                    'xyz_id': xyz_id,
                    'url': url,
                    'name': str(row.get('album_name', '')).strip() if pd.notna(row.get('album_name')) else None
                })
        
        logger.info(f"ä»ExcelåŠ è½½äº† {len(podcasts)} ä¸ªæ’­å®¢æ•°æ®ï¼Œå°†æµ‹è¯•å‰ {min(limit, len(podcasts))} ä¸ª")
        return podcasts[:limit]
        
    except Exception as e:
        logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        raise


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ‰¹é‡æµ‹è¯•æ’­å®¢æ•°æ®æŠ“å– - æ£€æµ‹æ•°æ®å¯ç”¨æ€§")
    print("=" * 60)
    print()
    
    # Excelæ–‡ä»¶è·¯å¾„
    excel_path = Path(__file__).parent.parent / "å°å®‡å®™ä¸“è¾‘èµ„æ–™-all.xlsx"
    
    if not excel_path.exists():
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return
    
    # åŠ è½½æ’­å®¢æ•°æ®
    print("ğŸ“– è¯»å–Excelæ–‡ä»¶...")
    try:
        podcasts = load_podcast_data_from_excel(str(excel_path), limit=100)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(podcasts)} ä¸ªæ’­å®¢æ•°æ®")
        print()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = BatchScraperTester()
    
    try:
        # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
        print("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        print(f"æµ‹è¯•æ•°é‡: {len(podcasts)}")
        print("å¹¶å‘æ•°: 5")
        print()
        
        start_time = datetime.now()
        results = await tester.test_batch(podcasts, limit=100)
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds()
        print(f"âœ… æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")
        print()
        
        # ç”ŸæˆæŠ¥å‘Š
        print("=" * 60)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        report = tester.generate_report()
        
        print(f"æ€»æµ‹è¯•æ•°: {report['total']}")
        print(f"å¯è®¿é—®: {report['accessible']} ({report['accessible_rate']})")
        print()
        print("æ•°æ®å¯ç”¨æ€§:")
        print(f"  - æœ‰æ ‡é¢˜: {report['data_availability']['has_title']} ({report['data_availability']['has_title_rate']})")
        print(f"  - æœ‰RSS: {report['data_availability']['has_rss']} ({report['data_availability']['has_rss_rate']})")
        print(f"  - æœ‰å°é¢: {report['data_availability']['has_cover']} ({report['data_availability']['has_cover_rate']})")
        print(f"  - æœ‰æè¿°: {report['data_availability']['has_description']} ({report['data_availability']['has_description_rate']})")
        print()
        print(f"é”™è¯¯æ•°: {report['errors_count']}")
        
        if report['error_samples']:
            print("\né”™è¯¯ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
            for error in report['error_samples']:
                print(f"  - {error.get('xyz_id', 'N/A')}: {error.get('error', 'Unknown error')}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
        output_file = "scraper_test_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump({
                "test_time": datetime.now().isoformat(),
                "total_tested": len(results),
                "report": report,
                "detailed_results": results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“„ æ—¥å¿—å·²ä¿å­˜åˆ°: scraper_test.log")
        
        # æ˜¾ç¤ºä¸€äº›æˆåŠŸç¤ºä¾‹
        successful = [r for r in results if r.get("accessible") and r.get("has_title")]
        if successful:
            print(f"\nâœ… æˆåŠŸç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for r in successful[:5]:
                print(f"  - {r['xyz_id']}: {r['data'].get('title', 'N/A')}")
        
    finally:
        await tester.close()


if __name__ == "__main__":
    asyncio.run(main())

