# 低并发、优化延迟、分时段执行策略

## 🎯 策略概述

**每天完成所有7000个播客的更新，使用低并发、优化延迟、分时段执行**

## 📊 核心策略

### 1. 低并发
- **并发数**: 8个（从20降低到8）
- **优势**: 降低被封禁风险，更稳定
- **劣势**: 速度稍慢，但更安全

### 2. 优化延迟
- **延迟范围**: 2-4秒（从3-6秒优化）
- **请求频率**: 每分钟15个请求（从10提高到15，但比30更保守）
- **优势**: 在安全和速度之间取得平衡

### 3. 分时段执行
- **方案A**: 单次执行（凌晨2点，低并发完成所有）
- **方案B**: 分8批执行（每小时一批，8小时完成）

## ⚙️ 配置详情

### 反爬虫配置

```python
optimized_config = {
    "rate_limiter": {
        "max_requests": 15,  # 每分钟15个请求（适中）
        "time_window": 60
    },
    "request_delay": {
        "min_delay": 2.0,    # 2-4秒延迟（保守但优化）
        "max_delay": 4.0,
        "base_delay": 3.0
    },
    "retry_strategy": {
        "max_attempts": 3,
        "initial_delay": 2.0,
        "max_delay": 30.0,
        "backoff_factor": 2.0,
        "jitter": True
    }
}
```

### 并发配置

```python
scrape_run = await scraper.scrape_all_podcasts_daily(
    max_concurrent=8  # 低并发（建议5-10）
)
```

## 📅 执行方案

### 方案A：单次执行（推荐）

**配置**：
- 执行时间：每天凌晨 2:00
- 并发数：8个
- 预计耗时：3-4小时

**时间计算**：
- 7000个播客 ÷ 8并发 = 875批
- 每批平均3秒 = 2625秒 ≈ **44分钟**（纯请求时间）
- 加上网络延迟、处理时间等，预计 **3-4小时**

**优势**：
- 简单，一次完成
- 排名计算在完成后统一进行

**劣势**：
- 需要连续运行3-4小时
- 如果中断需要重新开始

### 方案B：分时段执行（更稳妥）

**配置**：
- 分8批，每小时执行一批
- 每批约875个播客
- 从凌晨2点开始，到上午10点完成

**时间表**：
- 02:00 - 第1批（播客 1-875）
- 03:00 - 第2批（播客 876-1750）
- 04:00 - 第3批（播客 1751-2625）
- 05:00 - 第4批（播客 2626-3500）
- 06:00 - 第5批（播客 3501-4375）
- 07:00 - 第6批（播客 4376-5250）
- 08:00 - 第7批（播客 5251-6125）
- 09:00 - 第8批（播客 6126-7000）
- 10:00 - 统一计算排名

**优势**：
- 分散请求压力
- 降低被封禁风险
- 每批独立，失败不影响其他批次
- 可以在白天完成，便于监控

**劣势**：
- 需要8小时完成
- 排名计算在最后一批完成后进行

## 🔧 启用分时段执行

在 `scheduler.py` 的 `setup_scheduler()` 函数中，取消注释分时段执行的代码：

```python
def setup_scheduler():
    """设置定时任务"""
    # 方案1：单次执行（注释掉）
    # scheduler.add_job(...)
    
    # 方案2：分时段执行（取消注释）
    for i in range(8):  # 8批，每小时一批
        hour = (2 + i) % 24  # 从凌晨2点开始
        scheduler.add_job(
            lambda batch_idx=i: daily_scrape_task_batch(batch_idx, 8),
            trigger=CronTrigger(hour=hour, minute=0),
            id=f"daily_scrape_batch_{i}",
            name=f"每日播客数据抓取（批次 {i+1}/8）",
            replace_existing=True,
        )
```

## 📈 性能对比

| 方案 | 并发数 | 请求频率 | 延迟 | 预计耗时 | 风险 |
|------|--------|----------|------|----------|------|
| 高并发（原方案） | 20 | 30/分钟 | 1.5-3秒 | 1-2小时 | 较高 |
| **低并发单次** | **8** | **15/分钟** | **2-4秒** | **3-4小时** | **低** |
| **低并发分时段** | **8** | **15/分钟** | **2-4秒** | **8小时** | **最低** |

## 🚨 注意事项

### 1. 监控指标
- **成功率**: 应该 > 95%
- **错误类型**: 监控429、403等错误
- **平均耗时**: 每个播客 < 6秒

### 2. 如果速度太慢
可以适度调整（谨慎）：
- 并发数：8 → 10
- 请求频率：15 → 20
- 延迟：2-4秒 → 1.5-3秒

### 3. 如果遇到封禁
立即降低：
- 并发数：8 → 5
- 请求频率：15 → 10
- 延迟：2-4秒 → 3-6秒

## 💡 最佳实践

1. **首次运行**：使用单次执行方案，观察效果
2. **稳定后**：可以切换到分时段执行，更稳妥
3. **监控日志**：密切关注错误率和响应时间
4. **准备降级**：随时准备降低并发和频率

## 🔄 切换方案

### 从单次切换到分时段
1. 编辑 `scheduler.py`
2. 注释掉单次执行的代码
3. 取消注释分时段执行的代码
4. 重启服务

### 从分时段切换到单次
1. 编辑 `scheduler.py`
2. 注释掉分时段执行的代码
3. 取消注释单次执行的代码
4. 重启服务

