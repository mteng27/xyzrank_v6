# çˆ¬è™«ä½“ç³»å¼€å‘æŒ‡å—

> æœ¬æ–‡æ¡£ä¸“é—¨è®°å½•çˆ¬è™«ä½“ç³»çš„å¼€å‘è®¡åˆ’ã€æŠ€æœ¯æ–¹æ¡ˆå’Œå®ç°ç»†èŠ‚ã€‚

---

## ğŸ“‹ å½“å‰çŠ¶æ€

### âœ… å·²å®Œæˆ

1. **åŸºç¡€æ¶æ„**
   - `PodcastScraper` ç±»æ¡†æ¶
   - çˆ¬è™«APIæ¥å£
   - å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
   - æ•°æ®æ¨¡å‹ï¼ˆScrapeRunï¼‰

2. **åŸºç¡€åŠŸèƒ½**
   - `scrape_podcast_info()` - æŠ“å–æ’­å®¢åŸºæœ¬ä¿¡æ¯ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰
   - `update_podcast_from_scrape()` - æ›´æ–°æ’­å®¢ä¿¡æ¯
   - `record_daily_metric()` - è®°å½•æ¯æ—¥æŒ‡æ ‡
   - `scrape_all_podcasts()` - æ‰¹é‡æŠ“å–æ¡†æ¶

3. **æµ‹è¯•å·¥å…·**
   - `test_scraper_batch.py` - æ‰¹é‡æµ‹è¯•è„šæœ¬

### âš ï¸ å¾…å®ç°

1. **æ ¸å¿ƒåŠŸèƒ½**
   - `scrape_subscriber_count()` - æŠ“å–è®¢é˜…è€…æ•°é‡ï¼ˆ**å…³é”®åŠŸèƒ½ï¼Œå¾…å®ç°**ï¼‰
   - é¡µé¢è§£æé€»è¾‘ä¼˜åŒ–ï¼ˆéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´ï¼‰

2. **é«˜çº§åŠŸèƒ½**
   - åçˆ¬è™«ç­–ç•¥
   - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
   - å¹¶å‘æ§åˆ¶ä¼˜åŒ–
   - å¢é‡æ›´æ–°

---

## ğŸ¯ å¼€å‘ç›®æ ‡

### é˜¶æ®µä¸€ï¼šæ ¸å¿ƒåŠŸèƒ½å®ç°

**ç›®æ ‡**: å®ç°è®¢é˜…æ•°æŠ“å–åŠŸèƒ½ï¼Œä½¿çˆ¬è™«ç³»ç»Ÿèƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚

**ä»»åŠ¡æ¸…å•**:
1. [ ] åˆ†æå°å®‡å®™å¹³å°çš„è®¢é˜…æ•°è·å–æ–¹å¼
   - æ£€æŸ¥æ˜¯å¦æœ‰å…¬å¼€API
   - åˆ†æé¡µé¢ç»“æ„ï¼ˆé™æ€/åŠ¨æ€ï¼‰
   - ç¡®å®šæ•°æ®è·å–æ–¹æ³•

2. [ ] å®ç° `scrape_subscriber_count()` æ–¹æ³•
   - æ–¹æ¡ˆA: å¦‚æœå­˜åœ¨APIï¼Œå®ç°APIè°ƒç”¨
   - æ–¹æ¡ˆB: å¦‚æœé¡µé¢é™æ€ï¼Œä½¿ç”¨BeautifulSoupè§£æ
   - æ–¹æ¡ˆC: å¦‚æœé¡µé¢åŠ¨æ€ï¼Œä½¿ç”¨Playwright/Selenium

3. [ ] ä¼˜åŒ– `scrape_podcast_info()` æ–¹æ³•
   - æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´è§£æè§„åˆ™
   - å¢åŠ å®¹é”™æœºåˆ¶
   - å¤„ç†é¡µé¢ç»“æ„å˜åŒ–

4. [ ] æµ‹è¯•å’ŒéªŒè¯
   - æµ‹è¯•å•ä¸ªæ’­å®¢æŠ“å–
   - æµ‹è¯•æ‰¹é‡æŠ“å–ï¼ˆå°è§„æ¨¡ï¼‰
   - éªŒè¯æ•°æ®å‡†ç¡®æ€§

### é˜¶æ®µäºŒï¼šä½“ç³»å®Œå–„

**ç›®æ ‡**: å®Œå–„çˆ¬è™«ä½“ç³»ï¼Œæé«˜ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**ä»»åŠ¡æ¸…å•**:
1. [ ] åçˆ¬è™«ç­–ç•¥
   - è¯·æ±‚é¢‘ç‡æ§åˆ¶
   - User-Agent è½®æ¢
   - è¯·æ±‚å¤´è®¾ç½®
   - Cookie/Session ç®¡ç†ï¼ˆå¦‚éœ€è¦ï¼‰
   - IPä»£ç†æ± ï¼ˆå¦‚éœ€è¦ï¼‰

2. [ ] å¹¶å‘æ§åˆ¶ä¼˜åŒ–
   - å¯é…ç½®å¹¶å‘æ•°
   - åŠ¨æ€è°ƒæ•´å¹¶å‘ç­–ç•¥
   - è¯·æ±‚é˜Ÿåˆ—ç®¡ç†

3. [ ] é”™è¯¯å¤„ç†å’Œé‡è¯•
   - ç½‘ç»œé”™è¯¯é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
   - è§£æé”™è¯¯å¤„ç†
   - å¤±è´¥è®°å½•å’ŒæŠ¥å‘Š
   - å¼‚å¸¸æ•°æ®æ ‡è®°

4. [ ] å¢é‡æ›´æ–°
   - åªæŠ“å–éœ€è¦æ›´æ–°çš„æ’­å®¢
   - æ™ºèƒ½åˆ¤æ–­æ›´æ–°é¢‘ç‡
   - é¿å…é‡å¤æŠ“å–

### é˜¶æ®µä¸‰ï¼šç›‘æ§å’Œä¼˜åŒ–

**ç›®æ ‡**: æ·»åŠ ç›‘æ§åŠŸèƒ½ï¼Œä¼˜åŒ–æ€§èƒ½ã€‚

**ä»»åŠ¡æ¸…å•**:
1. [ ] çˆ¬å–ç›‘æ§
   - å®æ—¶æŸ¥çœ‹çˆ¬å–è¿›åº¦
   - æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
   - æ€§èƒ½æŒ‡æ ‡ï¼ˆè€—æ—¶ã€é€Ÿåº¦ç­‰ï¼‰

2. [ ] æ—¥å¿—ç³»ç»Ÿ
   - è¯¦ç»†çš„çˆ¬å–æ—¥å¿—
   - é”™è¯¯æ—¥å¿—åˆ†ç±»
   - æ—¥å¿—æŸ¥è¯¢å’Œåˆ†æ

3. [ ] å‘Šè­¦æœºåˆ¶
   - çˆ¬å–å¤±è´¥å‘Šè­¦
   - æ•°æ®å¼‚å¸¸å‘Šè­¦
   - ç³»ç»Ÿå¼‚å¸¸å‘Šè­¦

---

## ğŸ” æŠ€æœ¯æ–¹æ¡ˆåˆ†æ

### 1. è®¢é˜…æ•°è·å–æ–¹å¼

#### æ–¹æ¡ˆA: APIè°ƒç”¨

**ä¼˜ç‚¹**:
- é€Ÿåº¦å¿«
- æ•°æ®æ ¼å¼è§„èŒƒ
- ç¨³å®šæ€§é«˜

**ç¼ºç‚¹**:
- å¯èƒ½éœ€è¦è®¤è¯
- APIå¯èƒ½ä¸ç¨³å®šæˆ–å˜æ›´

**å®ç°æ­¥éª¤**:
1. åˆ†æå°å®‡å®™å¹³å°çš„ç½‘ç»œè¯·æ±‚
2. æ‰¾åˆ°è®¢é˜…æ•°ç›¸å…³çš„APIç«¯ç‚¹
3. å®ç°APIè°ƒç”¨é€»è¾‘
4. å¤„ç†è®¤è¯ï¼ˆå¦‚éœ€è¦ï¼‰

#### æ–¹æ¡ˆB: é™æ€é¡µé¢è§£æ

**ä¼˜ç‚¹**:
- ä¸éœ€è¦è®¤è¯
- å®ç°ç®€å•

**ç¼ºç‚¹**:
- é¡µé¢ç»“æ„å¯èƒ½å˜åŒ–
- éœ€è¦å¤„ç†å„ç§é¡µé¢æ ¼å¼

**å®ç°æ­¥éª¤**:
1. åˆ†æé¡µé¢HTMLç»“æ„
2. ä½¿ç”¨BeautifulSoupè§£æ
3. æå–è®¢é˜…æ•°æ•°æ®
4. å¤„ç†å„ç§é¡µé¢æ ¼å¼

#### æ–¹æ¡ˆC: åŠ¨æ€é¡µé¢æ¸²æŸ“

**ä¼˜ç‚¹**:
- å¯ä»¥å¤„ç†JavaScriptæ¸²æŸ“çš„å†…å®¹
- æ›´æ¥è¿‘çœŸå®æµè§ˆå™¨è¡Œä¸º

**ç¼ºç‚¹**:
- æ€§èƒ½è¾ƒä½
- èµ„æºæ¶ˆè€—å¤§

**å®ç°æ­¥éª¤**:
1. ä½¿ç”¨Playwrightæˆ–Selenium
2. ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
3. æå–è®¢é˜…æ•°æ•°æ®
4. ä¼˜åŒ–æ€§èƒ½ï¼ˆæ— å¤´æ¨¡å¼ã€èµ„æºè¿‡æ»¤ç­‰ï¼‰

### 2. åçˆ¬è™«ç­–ç•¥

#### è¯·æ±‚é¢‘ç‡æ§åˆ¶

```python
import asyncio
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = time_window  # ç§’
        self.requests = []
    
    async def acquire(self):
        now = datetime.now()
        # æ¸…ç†è¿‡æœŸè¯·æ±‚
        self.requests = [r for r in self.requests if now - r < timedelta(seconds=self.time_window)]
        
        if len(self.requests) >= self.max_requests:
            # ç­‰å¾…
            sleep_time = self.time_window - (now - self.requests[0]).total_seconds()
            await asyncio.sleep(sleep_time)
        
        self.requests.append(now)
```

#### User-Agent è½®æ¢

```python
import random

USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    # ... æ›´å¤šUser-Agent
]

def get_random_user_agent():
    return random.choice(USER_AGENTS)
```

#### è¯·æ±‚å¤´è®¾ç½®

```python
headers = {
    "User-Agent": get_random_user_agent(),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
import asyncio
from functools import wraps

def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_attempts):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        wait_time = delay * (backoff ** attempt)
                        await asyncio.sleep(wait_time)
                    else:
                        raise
            raise last_exception
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@retry(max_attempts=3, delay=1.0, backoff=2.0)
async def scrape_with_retry(xyz_id: str):
    # çˆ¬å–é€»è¾‘
    pass
```

### 4. å¹¶å‘æ§åˆ¶

```python
import asyncio

class ConcurrencyController:
    def __init__(self, max_concurrent: int = 5):
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def execute(self, coro):
        async with self.semaphore:
            return await coro

# ä½¿ç”¨ç¤ºä¾‹
controller = ConcurrencyController(max_concurrent=5)
tasks = [controller.execute(scrape_podcast(p)) for p in podcasts]
results = await asyncio.gather(*tasks)
```

---

## ğŸ“ ä»£ç å®ç°å»ºè®®

### 1. æ”¹è¿› `scrape_podcast_info()` æ–¹æ³•

```python
async def scrape_podcast_info(self, xyz_id: str) -> Optional[dict]:
    """
    æŠ“å–æ’­å®¢åŸºæœ¬ä¿¡æ¯ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    """
    try:
        url = f"https://www.xiaoyuzhou.fm/podcast/{xyz_id}"
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "zh-CN,zh;q=0.9",
        }
        
        response = await self.client.get(url, headers=headers)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        info = {}
        
        # å¤šç§æ–¹å¼å°è¯•è·å–åç§°
        title_tag = soup.find("title")
        if title_tag:
            info["name"] = title_tag.get_text().strip()
        
        # å°è¯•ä» meta æ ‡ç­¾è·å–
        og_title = soup.find("meta", {"property": "og:title"})
        if og_title and not info.get("name"):
            info["name"] = og_title.get("content", "").strip()
        
        # ç±»ä¼¼åœ°å¤„ç†å…¶ä»–å­—æ®µ...
        
        return info
        
    except Exception as e:
        logger.error(f"æŠ“å–æ’­å®¢ {xyz_id} ä¿¡æ¯å¤±è´¥: {e}")
        return None
```

### 2. å®ç° `scrape_subscriber_count()` æ–¹æ³•

```python
async def scrape_subscriber_count(self, xyz_id: str) -> Optional[int]:
    """
    æŠ“å–æ’­å®¢è®¢é˜…è€…æ•°é‡
    
    éœ€è¦æ ¹æ®å®é™…å¹³å°å®ç°ï¼š
    1. å¦‚æœå­˜åœ¨APIï¼Œè°ƒç”¨API
    2. å¦‚æœé¡µé¢é™æ€ï¼Œè§£æHTML
    3. å¦‚æœé¡µé¢åŠ¨æ€ï¼Œä½¿ç”¨Playwright
    """
    try:
        # æ–¹æ¡ˆ1: APIè°ƒç”¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # api_url = f"https://api.xiaoyuzhou.fm/podcast/{xyz_id}/stats"
        # response = await self.client.get(api_url)
        # data = response.json()
        # return data.get("subscriber_count")
        
        # æ–¹æ¡ˆ2: é™æ€é¡µé¢è§£æ
        url = f"https://www.xiaoyuzhou.fm/podcast/{xyz_id}"
        response = await self.client.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„è§£æè®¢é˜…æ•°
        # ç¤ºä¾‹ï¼šæŸ¥æ‰¾åŒ…å«"è®¢é˜…"æ–‡æœ¬çš„å…ƒç´ 
        subscriber_elements = soup.find_all(string=lambda text: text and "è®¢é˜…" in text)
        # è¿›ä¸€æ­¥è§£æ...
        
        # æ–¹æ¡ˆ3: åŠ¨æ€é¡µé¢æ¸²æŸ“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # from playwright.async_api import async_playwright
        # async with async_playwright() as p:
        #     browser = await p.chromium.launch(headless=True)
        #     page = await browser.new_page()
        #     await page.goto(url)
        #     # ç­‰å¾…å†…å®¹åŠ è½½
        #     await page.wait_for_selector("...")
        #     # æå–è®¢é˜…æ•°
        #     count = await page.text_content("...")
        #     await browser.close()
        
        return None  # å¾…å®ç°
        
    except Exception as e:
        logger.error(f"æŠ“å–æ’­å®¢ {xyz_id} è®¢é˜…è€…æ•°é‡å¤±è´¥: {e}")
        return None
```

### 3. æ”¹è¿› `scrape_all_podcasts()` æ–¹æ³•

```python
async def scrape_all_podcasts(self) -> ScrapeRun:
    """
    æŠ“å–æ‰€æœ‰æ’­å®¢çš„æ•°æ®ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    """
    scrape_run = ScrapeRun(status="running", started_at=datetime.now())
    self.session.add(scrape_run)
    await self.session.commit()
    await self.session.refresh(scrape_run)
    
    try:
        result = await self.session.execute(select(Podcast))
        podcasts = result.scalars().all()
        
        scrape_run.total_podcasts = len(podcasts)
        successful_count = 0
        failed_count = 0
        today = date.today()
        
        # å¹¶å‘æ§åˆ¶
        semaphore = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘
        
        async def process_podcast(podcast: Podcast):
            async with semaphore:
                try:
                    # æ›´æ–°æ’­å®¢ä¿¡æ¯
                    await self.update_podcast_from_scrape(podcast)
                    
                    # æŠ“å–è®¢é˜…è€…æ•°é‡
                    subscriber_count = await self.scrape_subscriber_count(podcast.xyz_id)
                    if subscriber_count is not None:
                        await self.record_daily_metric(
                            podcast.id, today, subscriber_count
                        )
                        return True
                    return False
                except Exception as e:
                    logger.error(f"å¤„ç†æ’­å®¢ {podcast.xyz_id} æ—¶å‡ºé”™: {e}")
                    return False
        
        # æ‰¹é‡å¤„ç†
        tasks = [process_podcast(p) for p in podcasts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        for result in results:
            if result is True:
                successful_count += 1
            else:
                failed_count += 1
        
        scrape_run.status = "completed"
        scrape_run.completed_at = datetime.now()
        scrape_run.successful_count = successful_count
        scrape_run.failed_count = failed_count
        
    except Exception as e:
        scrape_run.status = "failed"
        scrape_run.completed_at = datetime.now()
        scrape_run.error_message = str(e)
        logger.error(f"æ‰¹é‡æŠ“å–å¤±è´¥: {e}")
    
    await self.session.commit()
    await self.session.refresh(scrape_run)
    return scrape_run
```

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’

### 1. å•å…ƒæµ‹è¯•

- [ ] æµ‹è¯• `scrape_podcast_info()` æ–¹æ³•
- [ ] æµ‹è¯• `scrape_subscriber_count()` æ–¹æ³•
- [ ] æµ‹è¯• `record_daily_metric()` æ–¹æ³•
- [ ] æµ‹è¯•é”™è¯¯å¤„ç†é€»è¾‘

### 2. é›†æˆæµ‹è¯•

- [ ] æµ‹è¯•å•ä¸ªæ’­å®¢æŠ“å–æµç¨‹
- [ ] æµ‹è¯•æ‰¹é‡æŠ“å–æµç¨‹
- [ ] æµ‹è¯•å®šæ—¶ä»»åŠ¡è§¦å‘
- [ ] æµ‹è¯•APIæ¥å£

### 3. æ€§èƒ½æµ‹è¯•

- [ ] æµ‹è¯•å¹¶å‘æ€§èƒ½
- [ ] æµ‹è¯•å¤§é‡æ•°æ®æŠ“å–
- [ ] æµ‹è¯•èµ„æºæ¶ˆè€—

### 4. ç¨³å®šæ€§æµ‹è¯•

- [ ] é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
- [ ] é”™è¯¯æ¢å¤æµ‹è¯•
- [ ] ç½‘ç»œå¼‚å¸¸æµ‹è¯•

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **HTTPXæ–‡æ¡£**: https://www.python-httpx.org/
2. **BeautifulSoupæ–‡æ¡£**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
3. **Playwrightæ–‡æ¡£**: https://playwright.dev/python/
4. **APScheduleræ–‡æ¡£**: https://apscheduler.readthedocs.io/

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `backend/app/services/scraper_service.py` - çˆ¬è™«æœåŠ¡å®ç°
- `backend/app/api/scraper.py` - çˆ¬è™«APIæ¥å£
- `backend/app/tasks/scheduler.py` - å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
- `backend/test_scraper_batch.py` - æ‰¹é‡æµ‹è¯•è„šæœ¬
- `backend/app/models/podcast.py` - æ•°æ®æ¨¡å‹ï¼ˆå«ScrapeRunï¼‰

---

**æ–‡æ¡£ç»´æŠ¤**: è¯·åœ¨æ¯æ¬¡å®ç°æ–°åŠŸèƒ½åæ›´æ–°æœ¬æ–‡æ¡£ã€‚


